# 生成式人工智慧與異質平台整合應用 欉振坤教授  完整書面報告 
### 11463104蘇彥華 日期11/11
-------------------------------------
## 心得報告
聽完欉振坤教授的演講後，我清楚的認知到了GPT的各個層面，從GPT是如何從1代演化到現在5代的硬體或資料集訓練天數及調校知識等等，了解了他為何跟現在生活密不可分，但有優點就一定有缺點，在生成文字方便的背後，其實都是以機率的方式再生成，所以答案不是正確答案，在現在的生成式模型裡都有的通病，要如何去判斷並且不抄襲其他論文提出的論點，是我們這些現代人要解決的問題。在學習生成模型的過程中，我對GAN與GPT所使用的Transformer架構有了更深的理解。這兩種模型都是現代AI的重要技術，但它們的概念、架構與應用方式卻截然不同。

GAN的核心概念是「對抗式學習」——生成器（Generator）與鑑別器（Discriminator）互相競爭。生成器努力產生越來越逼真的資料，而鑑別器則嘗試分辨真假。透過這種動態博弈，模型逐漸提升生成能力。我認為 GAN的特點是直觀、強調創造力，尤其在影像生成與風格轉換領域非常成功，但訓練過程也相對不穩定。

相比之下，GPT 的 Transformer 架構則以注意力機制（Attention）為核心，模型能同時關注序列中不同位置的資訊。GPT使用Decoder-only的Transformer，擅長做語言建模與上下文推理。我覺得Transformer的設計比GAN更穩定，擴充性也更強，能透過大量資料與參數堆疊達成非常高的語言理解與生成能力。

總結GAN擅長在沒有標籤的情況下生成逼真的資料，而GPT跟Transformer則具備強大的語言理解與推理能力。透過對這兩種模型的比較，我更加理解深度學習中不同生成技術的設計理念與適用範圍，也對未來進一步研究生成式 AI 有了更清晰的方向。
## 關鍵字
### GAN介紹
生成式對抗網路 (GAN) 是一種深度學習架構。它訓練兩個神經網路來彼此競爭，從指定的訓練資料集中產生更真實的新資料。例如，您可以從現有的影像資料庫中產生新影像，或從歌曲資料庫中產生原始音樂。GAN 之所以被認為具有對抗性，是因為它可以訓練兩個不同的網路並使它們相互對抗。一個網路採取輸入資料範例，並儘可能多地修改來產生新資料。另一個網路會嘗試預測產生的資料輸出是否屬於原始資料集。換言之，預測網路確定產生的資料是虛假還是真實。系統會產生較新的改進版假資料值，直至預測網路無法再區分虛假資料與原始資料。
### GAN運作模式
1.產生器神經網絡神經網路分析訓練集並識別資料屬性  
2.鑑別器神經網路分析初始訓練資料並獨立區分屬性  
3.產生器透過向某些屬性新增雜訊 (或隨機變化) 來修改某些資料屬性  
4.產生器將修改的資料傳遞給鑑別器  
5.鑑別器計算產生輸出屬於原始資料集的概率  
6.鑑別器為產生器提供一些指導，以減少下一個週期中雜訊向量隨機化  
![2025-11-17 141420.png](https://github.com/Walter-sue/11463104/blob/main/images/2025-11-17%20141420.png)
### Transformer-based models
Transformer是一種基於多頭注意力機制的人工神經網路架構。在該架構中，文字被轉換為稱為「詞元」（token）的數值表示，每個詞元透過詞嵌入表查找轉換為向量。在每一層，每個詞元都會在上下文視窗的範圍內與其他（未被屏蔽的）詞元進行關聯，這種關聯是透過並行的多頭注意力機制實現的，從而放大關鍵字元的訊號，同時減弱不太重要的詞元。
![Transformer,_full_architecture.png](https://github.com/Walter-sue/11463104/blob/main/images/Transformer%2C_full_architecture.png)
### GPT介紹
基於轉換器的生成式預訓練模型是一種大型語言模型（LLM）[2][3][4]，也是生成式人工智慧的重要框架。首個GPT由OpenAI於2018年推出。GPT模型是基於Transformer模型的類神經網路，在大型未標記文字資料集上進行預訓練，並能夠生成類似於人類自然語言的文字。截至2023年，大多數LLM都具備這些特徵，並廣泛被稱為GPT。
## 參考文獻
https://aws.amazon.com/tw/what-is/gan/ 什麼是 GAN？  
https://zh.wikipedia.org/zh-tw/GPT_(%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)  
https://en.wikipedia.org/wiki/Transformer_(deep_learning)  
